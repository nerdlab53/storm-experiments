{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c3cf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1916,  0.2263,  0.6764,  0.3331,  0.2804,  0.1842],\n",
       "         [-1.0985, -0.7799, -0.2834, -1.5441, -0.9372,  0.7191],\n",
       "         [-0.3629,  0.5302,  1.9323, -0.6160,  0.5157,  2.6066],\n",
       "         [-0.4029,  0.8228,  0.2439,  0.8623,  0.0858,  0.0284]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.distributions as distributions\n",
    "import torch\n",
    "\n",
    "logits = torch.randn(1, 4, 6)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d43eb36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical(logits: torch.Size([1, 4, 6]))\n",
      "tensor([[2, 5, 5, 2]])\n",
      "tensor([[2, 5, 5, 3]])\n"
     ]
    }
   ],
   "source": [
    "sample = distributions.Categorical(logits=logits)\n",
    "print(sample)\n",
    "print(sample.sample())\n",
    "print(sample.probs.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e124af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n",
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "S = torch.tensor([5])\n",
    "norm_ratio = torch.max(torch.ones(1), S)\n",
    "print(S)\n",
    "print(norm_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "618a1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymLogTwoHotLoss()\n",
      "tensor([[[ 1.8980e+00, -1.6085e+00,  8.4599e-01, -7.6679e-01,  6.5921e-01,\n",
      "          -3.8726e-01,  8.4523e-01, -3.9936e-01,  1.8783e+00,  1.0451e-01,\n",
      "           1.9588e+00, -1.3000e-01, -6.2812e-01,  1.0949e+00,  9.0249e-01,\n",
      "           5.6656e-01, -1.4412e-01, -2.9450e+00, -5.7787e-01,  1.2009e+00,\n",
      "          -6.8237e-01, -1.2234e+00, -1.1486e+00,  1.0034e+00,  4.2528e-01,\n",
      "          -3.3263e-01, -3.2194e-01,  1.0251e+00, -8.3089e-01, -1.3858e-01,\n",
      "           3.0711e-01, -1.2917e+00, -1.0172e+00, -3.0163e+00,  4.1699e-01,\n",
      "          -1.5841e-01,  2.3525e-01,  5.2670e-01,  1.2528e+00,  8.9015e-01,\n",
      "           6.1383e-01, -4.1630e-01, -1.0373e+00, -1.2937e+00,  6.4527e-02,\n",
      "           1.6181e+00, -5.3967e-01, -1.6379e+00,  4.4835e-01,  7.4905e-03,\n",
      "          -4.0924e-01, -1.5125e-01,  1.1645e+00, -1.1510e+00, -1.9637e+00,\n",
      "          -3.1762e-01,  9.6048e-01, -7.2908e-01, -6.4671e-01, -3.0405e-01,\n",
      "           8.4101e-01, -1.1952e+00, -2.2104e-01,  2.2338e+00,  7.8695e-01,\n",
      "           9.9505e-02,  1.3196e+00,  1.1532e-01, -9.2331e-02,  9.1816e-02,\n",
      "          -1.0990e+00,  8.2270e-01, -5.3938e-01, -3.6006e-01, -4.7699e-01,\n",
      "           1.3832e+00, -1.6751e+00, -6.2524e-01, -2.9965e+00, -6.9598e-01,\n",
      "           6.0415e-01, -1.2134e-02,  3.3461e-01, -1.3089e+00, -1.2476e+00,\n",
      "          -1.5020e+00,  5.3831e-01, -2.3426e+00, -9.9632e-01,  6.7196e-01,\n",
      "           1.6073e+00, -4.0182e-01, -1.2366e+00,  2.3471e+00,  8.1564e-01,\n",
      "           1.5523e-01,  2.6387e-01, -4.4538e-01, -6.9774e-01, -1.9946e-01,\n",
      "           6.0057e-01, -2.4267e-02,  2.4013e+00,  5.4974e-01, -7.2882e-01,\n",
      "          -1.5597e+00, -6.7384e-01, -2.6850e-01,  2.6543e-03,  4.3641e-01,\n",
      "          -1.1585e-01,  1.1109e-01, -9.4398e-01, -1.3082e+00, -2.1411e-02,\n",
      "           2.0019e-01, -1.9641e+00, -5.3932e-01, -1.3304e+00,  1.4093e+00,\n",
      "          -2.9106e-01,  4.4958e-01, -1.2494e+00, -5.9285e-01, -3.0108e-01,\n",
      "           5.2208e-01,  1.7313e+00, -3.4934e-02,  5.0367e-01,  7.8367e-02,\n",
      "           1.3154e+00, -1.1145e-01,  1.2634e+00, -5.4128e-01,  7.8448e-01,\n",
      "          -1.6244e+00,  1.7588e-01,  6.4070e-02, -3.4283e-01,  2.1531e-02,\n",
      "           5.5468e-01,  1.9412e-01,  1.6149e+00, -1.6815e-01, -8.7252e-01,\n",
      "           1.0238e+00,  1.4229e-01, -1.0363e+00,  1.5825e+00,  8.7584e-01,\n",
      "          -9.7375e-01, -2.0382e+00, -2.3726e+00, -6.4962e-02,  2.4771e-01,\n",
      "          -4.6033e-02,  2.0130e+00,  1.7392e+00,  1.2318e+00, -1.1489e+00,\n",
      "           4.2272e-01,  2.0747e-01,  4.3317e-01,  4.0518e-01,  1.1506e+00,\n",
      "          -7.0705e-01,  1.9662e-01, -1.7253e-01,  1.1265e+00, -1.2828e-02,\n",
      "           4.5466e-01,  7.2491e-01,  1.4553e+00,  1.4359e+00,  7.4778e-01,\n",
      "           6.7074e-01, -9.5164e-01,  9.5449e-01, -9.4959e-02,  8.2929e-01,\n",
      "          -1.1210e+00, -8.7864e-01, -6.6096e-02,  8.1010e-02,  4.4606e-01,\n",
      "          -5.0913e-01,  4.7231e-01, -8.2982e-01, -8.5919e-01, -1.7693e-01,\n",
      "           2.0815e-02, -8.8284e-02,  3.6892e-01, -1.6026e+00,  2.5960e+00,\n",
      "           9.2168e-01,  5.3185e-01, -1.7450e+00,  1.1020e+00,  4.7076e-01,\n",
      "           1.2863e+00,  2.4960e+00,  9.9535e-03,  9.3255e-01, -9.0104e-01,\n",
      "          -2.2259e-01,  7.9692e-01, -2.0004e+00,  1.4783e-01, -4.8376e-01,\n",
      "           1.2655e+00, -4.9860e-01, -1.8396e+00, -1.8328e-01,  9.9746e-01,\n",
      "          -9.5613e-01, -3.5252e-01, -2.4922e-02,  1.2268e+00,  8.0048e-01,\n",
      "          -1.0113e+00, -2.2235e-02,  8.7004e-01, -5.5290e-01, -3.6843e-01,\n",
      "           2.6659e-02,  5.3919e-01,  1.6981e+00,  9.4350e-01,  1.4222e-01,\n",
      "          -1.0021e-04,  3.8493e-01, -3.2440e+00,  1.9452e-01,  9.6819e-01,\n",
      "          -1.3727e+00, -3.5908e-01,  7.3653e-01, -8.1460e-01,  1.4828e+00,\n",
      "          -6.8906e-01, -4.1133e-01,  1.3174e+00, -1.9374e-01,  2.2249e+00,\n",
      "           1.0887e+00, -1.6213e-01, -6.7559e-01,  1.3682e+00,  6.8110e-01,\n",
      "          -6.5241e-01, -1.3961e+00, -9.0444e-01,  1.2849e+00, -6.7576e-02]]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.1000]])\n",
      "tensor(5.7763, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def symlog(x):\n",
    "    return torch.sign(x) * torch.log(1 + torch.abs(x))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def symexp(x):\n",
    "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)\n",
    "\n",
    "\n",
    "class SymLogLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        target = symlog(target)\n",
    "        return 0.5*F.mse_loss(output, target)\n",
    "\n",
    "\n",
    "class SymLogTwoHotLoss(nn.Module):\n",
    "    def __init__(self, num_classes, lower_bound, upper_bound):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.bin_length = (upper_bound - lower_bound) / (num_classes-1)\n",
    "\n",
    "        # use register buffer so that bins move with .cuda() automatically\n",
    "        self.bins: torch.Tensor\n",
    "        self.register_buffer(\n",
    "            'bins', torch.linspace(-20, 20, num_classes), persistent=False)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        target = symlog(target)\n",
    "        assert target.min() >= self.lower_bound and target.max() <= self.upper_bound\n",
    "\n",
    "        index = torch.bucketize(target, self.bins)\n",
    "        diff = target - self.bins[index-1]  # -1 to get the lower bound\n",
    "        weight = diff / self.bin_length\n",
    "        weight = torch.clamp(weight, 0, 1)\n",
    "        weight = weight.unsqueeze(-1)\n",
    "\n",
    "        target_prob = (1-weight)*F.one_hot(index-1, self.num_classes) + weight*F.one_hot(index, self.num_classes)\n",
    "\n",
    "        loss = -target_prob * F.log_softmax(output, dim=-1)\n",
    "        loss = loss.sum(dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "    def decode(self, output):\n",
    "        return symexp(F.softmax(output, dim=-1) @ self.bins)\n",
    "\n",
    "\n",
    "loss_func = SymLogTwoHotLoss(255, -20, 20)\n",
    "print(loss_func)\n",
    "output = torch.randn(1, 1, 255).requires_grad_()\n",
    "print(output)\n",
    "target = torch.ones(1).reshape(1, 1).float() * 0.1\n",
    "print(target)\n",
    "loss = loss_func(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075794e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
